{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vanilla GAN + DCGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xxxsarahfu/ECE9123-DL/blob/main/vanillaGAN%2BDCGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF2x3qooyBTI"
      },
      "source": [
        "# Generate handwritten digits by GANs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfIk2es3hJEd"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras.layers import Conv2D, Conv2DTranspose, ReLU, LeakyReLU, Dropout, Flatten, Dense, BatchNormalization, Reshape\n",
        "from keras import losses, optimizers\n",
        "from keras.models import Sequential, Model\n",
        "import time\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure, imshow, axis\n",
        "from matplotlib.image import imread\n",
        "import glob\n",
        "from IPython.display import Image, display\n",
        "import imageio\n",
        "\n",
        "os.makedirs(\"fake_images\", exist_ok=True) # create a folder for generated images\n",
        "matplotlib.use('Agg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYn4MdZnKCey"
      },
      "source": [
        "## Download dataset and pre-process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4PIDhoDLbsZ"
      },
      "source": [
        "# Parameters settings\n",
        "lr = 0.0002\n",
        "noise_dim = 100\n",
        "img_samples = 100\n",
        "\n",
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "EPOCHS = 40\n",
        "\n",
        "gan_type = 'DCGAN'\n",
        "#gan_type = 'vGAN' #uncomment this line to change gan type to vanilla gan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4fYMGxGhrna"
      },
      "source": [
        "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
        "train_images = (train_images - 127.5) / 127.5  # normalize the images to (-1, 1)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0IKnaCtg6WE"
      },
      "source": [
        "## Discriminator model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayIcTq-UNcmn"
      },
      "source": [
        "def Discriminator(type):\n",
        "  if type == 'vGAN':\n",
        "    return Discriminator_vn()\n",
        "  else:\n",
        "    return Discriminator_dc()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyObkRL-Oqu2"
      },
      "source": [
        "def Discriminator_vn():\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=[28, 28, 1]))\n",
        "    model.add(Dense(256))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(128))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(64))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dw2tPLmk2pEP"
      },
      "source": [
        "def Discriminator_dc():\n",
        "    model = Sequential()\n",
        "\n",
        "    # (1×28×28) --> (64×14×14)\n",
        "    model.add(Conv2D(64, (5, 5), strides=(1, 1), padding='same',input_shape=[28, 28, 1]))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(0.3))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    # (64×14×14) --> (128×7×7) \n",
        "    model.add(Conv2D(128, (5, 5), strides=(1, 1), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(0.3))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    # (128×7×7) -> (6272×1) -> (1)\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnQZLOyQOwHo"
      },
      "source": [
        "D = Discriminator(gan_type)\n",
        "D.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THY-sZMiQ4UV"
      },
      "source": [
        "## Generator model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGGLCY3QOFyU"
      },
      "source": [
        "def Generator(type):\n",
        "  if type == 'vGAN':\n",
        "    return Generator_vn()\n",
        "  else:\n",
        "    return Generator_dc()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YdHCDxGOSsN"
      },
      "source": [
        "def Generator_vn():\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(64, input_dim=noise_dim))\n",
        "    model.add(LeakyReLU(0.3))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Dense(128, input_dim=noise_dim))\n",
        "    model.add(LeakyReLU(0.3))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Dense(256))\n",
        "    model.add(LeakyReLU(0.3))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Dense(np.prod([28,28,1]), activation='tanh'))\n",
        "    model.add(Reshape([28, 28, 1]))\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bpTcDqoLWjY"
      },
      "source": [
        "def Generator_dc():\n",
        "    model = Sequential()\n",
        "    \n",
        "    # 100 --> (256×7×7)\n",
        "    model.add(Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(0.3))\n",
        "    model.add(Reshape((7, 7, 256)))\n",
        "    \n",
        "    # (256×7×7) --> (128×7×7), s=1, p=2\n",
        "    model.add(Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(0.3))\n",
        "\n",
        "    # (128×7×7) --> (64×14×14)\n",
        "    model.add(Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(0.3))\n",
        "    \n",
        "\n",
        "    # (64×14×14) --> (1×28×28)\n",
        "    model.add(Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QkJUUZkOb3k"
      },
      "source": [
        "G = Generator(gan_type)\n",
        "G.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw1fkAczTQYh"
      },
      "source": [
        "## Loss functions and Optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psQfmXxYKU3X"
      },
      "source": [
        "g_optimizer = optimizers.Adam(lr)\n",
        "d_optimizer = optimizers.Adam(lr)\n",
        "\n",
        "criterion = losses.BinaryCrossentropy()\n",
        "if gan_type == 'DCGAN':\n",
        "  criterion = losses.BinaryCrossentropy(from_logits=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkMNfBWlT-PV"
      },
      "source": [
        "def get_g_loss(fake_output):\n",
        "    return criterion(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "def get_d_loss(real_output, fake_output):\n",
        "    real_loss = criterion(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = criterion(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = (real_loss + fake_loss) / 2\n",
        "    return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZrd4CdjR-Fp"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NS2GWywBbAWo"
      },
      "source": [
        "# a fixed noise to feed in trained G model\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "z_seed = tf.random.normal([img_samples, noise_dim]) \n",
        "if gan_type == 'vGAN':\n",
        "  z_seed = np.random.normal(0, 1, (img_samples, noise_dim))\n",
        "  \n",
        "# record loss after each epoch\n",
        "losses_hist_g = []\n",
        "losses_hist_d = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2M7LmLtGEMQJ"
      },
      "source": [
        "# to_plot_epochs = [0,9,29,49]\n",
        "def train(dataset, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        dataset_len = len(dataset)\n",
        "        epoch_loss_g, epoch_loss_d = 0.0, 0.0\n",
        "        for image_batch in dataset:\n",
        "            # Train D and G\n",
        "            gen_loss, disc_loss = train_step(image_batch)\n",
        "            epoch_loss_g += gen_loss\n",
        "            epoch_loss_d += disc_loss\n",
        "\n",
        "        # Compute loss for each epoch\n",
        "        epoch_loss_g /= dataset_len\n",
        "        epoch_loss_d /= dataset_len\n",
        "        losses_hist_g.append(epoch_loss_g)\n",
        "        losses_hist_d.append(epoch_loss_d)\n",
        "        print (f\"Epoch {epoch+1} of {EPOCHS}:    Generator loss: {epoch_loss_g:.8f}, Discriminator loss: {epoch_loss_d:.8f}\")\n",
        "\n",
        "        # plot and save image using fixed noise and trained model\n",
        "        #if epoch in to_plot_epochs:\n",
        "        generate_and_save_images(epoch+1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3t5ibNo05jCB"
      },
      "source": [
        "# helper function for training batch-image for both D and G\n",
        "@tf.function\n",
        "def train_step(images, ):\n",
        "    # create noise\n",
        "    z = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "    if gan_type == 'vGAN':\n",
        "      z = np.random.normal(0, 1, (BATCH_SIZE, noise_dim))\n",
        "       \n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      gen_images = G(z, training=True)\n",
        "\n",
        "      real_out = D(images, training=True)\n",
        "      fake_out = D(gen_images, training=True)\n",
        "\n",
        "      gen_loss = get_g_loss(fake_out)\n",
        "      disc_loss = get_d_loss(real_out, fake_out)\n",
        "\n",
        "    grad_of_g = gen_tape.gradient(gen_loss, G.trainable_variables)\n",
        "    grad_of_d = disc_tape.gradient(disc_loss, D.trainable_variables)\n",
        "\n",
        "    g_optimizer.apply_gradients(zip(grad_of_g, G.trainable_variables))\n",
        "    d_optimizer.apply_gradients(zip(grad_of_d, D.trainable_variables))\n",
        "  \n",
        "    return gen_loss, disc_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmdVsmvhPxyy"
      },
      "source": [
        "# helper function to plot and save fake images\n",
        "def generate_and_save_images(epoch):\n",
        "    out = G(z_seed, training=False)\n",
        "\n",
        "    fig = plt.figure(figsize=(6, 6))\n",
        "    for i in range(out.shape[0]):\n",
        "        plt.subplot(10, 10, i+1)\n",
        "        plt.imshow(out[i, :, :, 0] * 0.5 + 0.5, cmap='gray')\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.savefig('fake_images/img_{:02d}.png'.format(epoch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly3UN0SLLY2l"
      },
      "source": [
        "# Main program\n",
        "print(\"============================= START TRAINING ==============================\")\n",
        "start = time.time()\n",
        "train(train_dataset, EPOCHS)\n",
        "time_cost = time.time() - start\n",
        "print(\"============================== END TRAINING ===============================\")\n",
        "print(f\"Total time cost: {time_cost:.2f} secs.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UN9Au4tTRq_"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owD6Uk4-TVC7"
      },
      "source": [
        "# Generate gif \n",
        "anim_file = 'gan.gif'\n",
        "filenames\n",
        "with imageio.get_writer(anim_file, mode='I') as writer:\n",
        "  filenames = glob.glob('fake_images/img*.png')\n",
        "  filenames = sorted(filenames)\n",
        "  for filename in filenames:\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArYApBgEFhjo"
      },
      "source": [
        "def showImagesHorizontally(list_of_files):\n",
        "    fig = plt.figure(figsize=(24, 6))\n",
        "    for i, img_name in enumerate(list_of_files):\n",
        "        a = plt.subplot(1, 4, i+1)\n",
        "        a.title.set_text('Epoch ' + img_name[-6:-4])\n",
        "        plt.imshow(imread(img_name), cmap='gray')\n",
        "        plt.axis('off')\n",
        "    fig.tight_layout()\n",
        "    fig.savefig('imgs_summary.png')\n",
        "\n",
        "filenames = ['fake_images/img_01.png','fake_images/img_05.png','fake_images/img_25.png']\n",
        "filenames = sorted(filenames)\n",
        "print(filenames)\n",
        "showImagesHorizontally(filenames)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql7Gy4KMY03h"
      },
      "source": [
        "## Loss curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5VOiMTWY0Fh"
      },
      "source": [
        "# plot and save the generator and discriminator loss\n",
        "plt.figure()\n",
        "plt.plot(losses_hist_g, label='Generator loss')\n",
        "plt.plot(losses_hist_d, label='Discriminator Loss')\n",
        "plt.title(f\"batch_size: {BATCH_SIZE}\\n time cost: {time_cost:.2f} secs.\")\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.savefig('loss.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}